{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kambl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kambl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kambl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Machine learning is the study of computer algorithms that improve automatically through experience and by the use of data.\n",
    "It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data,\n",
    "known as training data, in order to make predictions or decisions without being explicitly programmed to do so.\n",
    "\"\"\"\n",
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "Machine learning is the study of computer algorithms that improve automatically through experience and by the use of data.\n",
      "It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data,\n",
      "known as training data, in order to make predictions or decisions without being explicitly programmed to do so.\n",
      "\n",
      "\n",
      "Processed Text:\n",
      " machin learn studi comput algorithm improv automat experi use data . seen part artifici intellig . machin learn algorithm build model base sampl data , known train data , order make predict decis without explicitli program .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\\n\", text)\n",
    "print(\"\\nProcessed Text:\\n\", processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'is',\n",
       " 'an',\n",
       " 'essential',\n",
       " 'step',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'It',\n",
       " 'includes',\n",
       " 'tasks',\n",
       " 'like',\n",
       " 'stop',\n",
       " 'word',\n",
       " 'removal',\n",
       " ',',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'lemmatization',\n",
       " ',',\n",
       " 'and',\n",
       " 'handling',\n",
       " 'punctuation',\n",
       " '.',\n",
       " 'These',\n",
       " 'steps',\n",
       " 'help',\n",
       " 'clean',\n",
       " 'and',\n",
       " 'normalize',\n",
       " 'the',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'making',\n",
       " 'it',\n",
       " 'more',\n",
       " 'suitable',\n",
       " 'for',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'improving',\n",
       " 'the',\n",
       " 'performance',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'models',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_document = \"\"\"Text preprocessing is an essential step in natural language processing. It includes tasks like stop word removal,\n",
    "tokenization, stemming, lemmatization, and handling punctuation. These steps help clean and normalize the text data, making it more suitable for\n",
    "analysis and improving the performance of NLP models.\"\"\"\n",
    "words = word_tokenize(text_document)\n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'essential',\n",
       " 'step',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'includes',\n",
       " 'tasks',\n",
       " 'like',\n",
       " 'stop',\n",
       " 'word',\n",
       " 'removal',\n",
       " ',',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'lemmatization',\n",
       " ',',\n",
       " 'handling',\n",
       " 'punctuation',\n",
       " '.',\n",
       " 'steps',\n",
       " 'help',\n",
       " 'clean',\n",
       " 'normalize',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'making',\n",
       " 'suitable',\n",
       " 'analysis',\n",
       " 'improving',\n",
       " 'performance',\n",
       " 'NLP',\n",
       " 'models',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'preprocess',\n",
       " 'essenti',\n",
       " 'step',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " '.',\n",
       " 'includ',\n",
       " 'task',\n",
       " 'like',\n",
       " 'stop',\n",
       " 'word',\n",
       " 'remov',\n",
       " ',',\n",
       " 'token',\n",
       " ',',\n",
       " 'stem',\n",
       " ',',\n",
       " 'lemmat',\n",
       " ',',\n",
       " 'handl',\n",
       " 'punctuat',\n",
       " '.',\n",
       " 'step',\n",
       " 'help',\n",
       " 'clean',\n",
       " 'normal',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'make',\n",
       " 'suitabl',\n",
       " 'analysi',\n",
       " 'improv',\n",
       " 'perform',\n",
       " 'nlp',\n",
       " 'model',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "stemmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'essential',\n",
       " 'step',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'includes',\n",
       " 'task',\n",
       " 'like',\n",
       " 'stop',\n",
       " 'word',\n",
       " 'removal',\n",
       " ',',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'lemmatization',\n",
       " ',',\n",
       " 'handling',\n",
       " 'punctuation',\n",
       " '.',\n",
       " 'step',\n",
       " 'help',\n",
       " 'clean',\n",
       " 'normalize',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'making',\n",
       " 'suitable',\n",
       " 'analysis',\n",
       " 'improving',\n",
       " 'performance',\n",
       " 'NLP',\n",
       " 'model',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "lemmatized_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
